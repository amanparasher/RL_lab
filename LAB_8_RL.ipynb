{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwZGUF7045J24crqlwhZIx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# To Implement Monte Carlo Prediction"],"metadata":{"id":"eS3l3mxfUVkP"}},{"cell_type":"code","source":["import numpy as np\n","import gym\n","from collections import defaultdict\n","\n","def monte_carlo_prediction(env, num_episodes, gamma=1.0):\n","    # Initialize value function arbitrarily\n","    V = defaultdict(float)\n","    returns_sum = defaultdict(float)\n","    returns_count = defaultdict(float)\n","\n","    for episode in range(num_episodes):\n","        episode_states = []\n","        episode_rewards = []\n","\n","        # Generate an episode\n","        state = env.reset()\n","        done = False\n","        while not done:\n","            episode_states.append(state)\n","            action = env.action_space.sample()  # Random policy\n","            next_state, reward, done, _ = env.step(action)\n","            episode_rewards.append(reward)\n","            state = next_state\n","\n","        # Update value function for each state in the episode\n","        G = 0\n","        for t in range(len(episode_states) - 1, -1, -1):\n","            state = episode_states[t]\n","            G = gamma * G + episode_rewards[t]\n","            if state not in episode_states[:t]:\n","                returns_sum[state] += G\n","                returns_count[state] += 1\n","                V[state] = returns_sum[state] / returns_count[state]\n","\n","    return V\n","\n","# Example usage\n","env = gym.make('Blackjack-v1')\n","num_episodes = 1000\n","V = monte_carlo_prediction(env, num_episodes)\n","\n","# Print value function line by line\n","for state, value in V.items():\n","    print(f\"State: {state}, Value: {value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"G5HE13XLUvxE","executionInfo":{"status":"ok","timestamp":1712087029608,"user_tz":-330,"elapsed":610,"user":{"displayName":"AMAN PARASHER (RA2111047010157)","userId":"12589665428718136377"}},"outputId":"afe155e3-e607-4645-fc5f-f3e88c11ef64"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["State: (8, 2, False), Value: -1.0\n","State: (19, 3, False), Value: -1.0\n","State: (9, 10, False), Value: -0.18181818181818182\n","State: (19, 7, False), Value: 0.0\n","State: (13, 7, False), Value: -0.6923076923076923\n","State: (12, 7, False), Value: -0.5\n","State: (16, 10, False), Value: -0.6666666666666666\n","State: (15, 6, False), Value: 0.14285714285714285\n","State: (8, 10, False), Value: -0.45454545454545453\n","State: (12, 3, False), Value: -0.5384615384615384\n","State: (16, 7, False), Value: -1.0\n","State: (17, 9, False), Value: -0.6666666666666666\n","State: (15, 9, False), Value: -0.3333333333333333\n","State: (14, 9, False), Value: -1.0\n","State: (11, 9, False), Value: 0.0\n","State: (19, 2, False), Value: 0.0\n","State: (15, 3, False), Value: -0.2\n","State: (19, 10, False), Value: -0.3225806451612903\n","State: (20, 8, False), Value: 0.6363636363636364\n","State: (14, 8, False), Value: 0.14285714285714285\n","State: (20, 6, False), Value: 0.08333333333333333\n","State: (7, 8, False), Value: -1.0\n","State: (21, 10, True), Value: 0.45454545454545453\n","State: (15, 10, True), Value: -0.25\n","State: (12, 10, False), Value: -0.5476190476190477\n","State: (13, 10, False), Value: -0.56\n","State: (17, 1, False), Value: -0.8571428571428571\n","State: (18, 6, True), Value: 1.0\n","State: (18, 10, False), Value: -0.6764705882352942\n","State: (17, 3, False), Value: -0.5\n","State: (14, 7, False), Value: -0.4\n","State: (15, 10, False), Value: -0.6296296296296297\n","State: (18, 1, False), Value: -0.5555555555555556\n","State: (13, 2, True), Value: -1.0\n","State: (17, 10, False), Value: -0.88\n","State: (20, 10, False), Value: -0.25\n","State: (18, 10, True), Value: -0.25\n","State: (13, 4, False), Value: -0.5\n","State: (10, 4, False), Value: 0.0\n","State: (11, 2, False), Value: -0.3333333333333333\n","State: (9, 2, False), Value: -1.0\n","State: (14, 5, False), Value: -0.5\n","State: (15, 4, False), Value: -0.6666666666666666\n","State: (15, 4, True), Value: -1.0\n","State: (18, 3, False), Value: -0.45454545454545453\n","State: (21, 10, False), Value: 0.2222222222222222\n","State: (11, 10, False), Value: -0.36363636363636365\n","State: (20, 9, False), Value: -0.6363636363636364\n","State: (16, 8, False), Value: -0.2727272727272727\n","State: (11, 8, False), Value: -0.3333333333333333\n","State: (15, 7, False), Value: -0.3333333333333333\n","State: (10, 7, False), Value: 0.0\n","State: (9, 1, False), Value: -1.0\n","State: (14, 6, False), Value: -0.5\n","State: (18, 2, False), Value: -0.14285714285714285\n","State: (17, 2, False), Value: -0.375\n","State: (19, 8, False), Value: -0.4166666666666667\n","State: (17, 1, True), Value: 0.0\n","State: (7, 5, False), Value: -1.0\n","State: (21, 6, True), Value: 0.5\n","State: (14, 10, False), Value: -0.6551724137931034\n","State: (16, 10, True), Value: -0.5\n","State: (17, 7, True), Value: -0.5\n","State: (20, 4, False), Value: 0.5\n","State: (6, 10, False), Value: -0.5\n","State: (10, 1, False), Value: -1.0\n","State: (21, 3, False), Value: -0.5\n","State: (20, 3, False), Value: -0.38461538461538464\n","State: (17, 5, False), Value: -0.3333333333333333\n","State: (13, 7, True), Value: 0.0\n","State: (20, 2, False), Value: -0.6363636363636364\n","State: (13, 1, False), Value: -0.38461538461538464\n","State: (10, 3, False), Value: -0.6\n","State: (16, 5, False), Value: -0.6\n","State: (17, 6, False), Value: -0.14285714285714285\n","State: (7, 6, False), Value: -0.3333333333333333\n","State: (14, 10, True), Value: -0.14285714285714285\n","State: (18, 8, False), Value: -0.75\n","State: (21, 4, False), Value: 0.0\n","State: (21, 4, True), Value: 1.0\n","State: (19, 9, False), Value: -0.25\n","State: (10, 5, False), Value: 0.42857142857142855\n","State: (8, 5, False), Value: -0.25\n","State: (21, 8, False), Value: -0.5\n","State: (21, 8, True), Value: 1.0\n","State: (17, 8, True), Value: 0.5\n","State: (16, 3, False), Value: -0.7142857142857143\n","State: (13, 3, False), Value: -0.25\n","State: (18, 6, False), Value: -0.36363636363636365\n","State: (15, 2, False), Value: -0.6\n","State: (13, 2, False), Value: -0.6363636363636364\n","State: (12, 1, False), Value: -1.0\n","State: (18, 5, False), Value: -0.2\n","State: (13, 9, False), Value: -0.7142857142857143\n","State: (14, 9, True), Value: -1.0\n","State: (8, 6, False), Value: -0.5\n","State: (18, 7, False), Value: 0.14285714285714285\n","State: (20, 5, False), Value: -0.42857142857142855\n","State: (12, 5, False), Value: -0.38461538461538464\n","State: (16, 1, False), Value: -1.0\n","State: (9, 5, False), Value: 0.5\n","State: (19, 4, True), Value: 0.0\n","State: (8, 4, False), Value: -1.0\n","State: (12, 2, False), Value: -1.0\n","State: (20, 7, False), Value: -0.25\n","State: (19, 9, True), Value: 0.0\n","State: (10, 2, False), Value: -0.14285714285714285\n","State: (20, 10, True), Value: 0.0\n","State: (13, 8, False), Value: -0.16666666666666666\n","State: (4, 10, False), Value: 1.0\n","State: (7, 7, False), Value: -0.5\n","State: (15, 8, False), Value: -0.5833333333333334\n","State: (14, 1, False), Value: -0.7142857142857143\n","State: (19, 6, False), Value: -1.0\n","State: (14, 2, False), Value: -0.5\n","State: (13, 10, True), Value: -1.0\n","State: (17, 4, False), Value: -0.42857142857142855\n","State: (14, 4, False), Value: -0.75\n","State: (12, 9, False), Value: -1.0\n","State: (10, 9, False), Value: 0.0\n","State: (15, 5, False), Value: -0.5\n","State: (11, 6, False), Value: 0.3333333333333333\n","State: (10, 10, False), Value: -0.5\n","State: (19, 2, True), Value: 0.5\n","State: (11, 1, False), Value: -1.0\n","State: (21, 2, False), Value: 1.0\n","State: (21, 9, False), Value: 0.0\n","State: (11, 5, False), Value: 0.0\n","State: (16, 6, False), Value: -1.0\n","State: (21, 7, False), Value: 1.0\n","State: (17, 7, False), Value: -0.6\n","State: (21, 7, True), Value: 0.5\n","State: (5, 3, False), Value: 0.0\n","State: (21, 1, False), Value: 0.25\n","State: (19, 10, True), Value: -0.14285714285714285\n","State: (12, 6, False), Value: -0.42857142857142855\n","State: (11, 3, False), Value: -0.5\n","State: (9, 3, False), Value: 0.0\n","State: (4, 2, False), Value: -1.0\n","State: (17, 8, False), Value: -1.0\n","State: (21, 2, True), Value: 1.0\n","State: (16, 5, True), Value: -1.0\n","State: (13, 5, True), Value: -1.0\n","State: (4, 5, False), Value: -1.0\n","State: (17, 2, True), Value: -0.3333333333333333\n","State: (10, 8, False), Value: 0.2\n","State: (19, 7, True), Value: 0.0\n","State: (8, 7, False), Value: -0.5\n","State: (12, 4, False), Value: 0.2\n","State: (13, 5, False), Value: -0.25\n","State: (19, 4, False), Value: -0.09090909090909091\n","State: (16, 2, False), Value: -0.6666666666666666\n","State: (21, 6, False), Value: -0.3333333333333333\n","State: (14, 3, False), Value: -0.25\n","State: (14, 3, True), Value: -1.0\n","State: (16, 8, True), Value: -1.0\n","State: (21, 9, True), Value: -0.2\n","State: (18, 9, False), Value: -0.2\n","State: (19, 1, False), Value: -0.6\n","State: (16, 3, True), Value: -1.0\n","State: (12, 8, False), Value: -0.8\n","State: (13, 6, True), Value: -1.0\n","State: (21, 1, True), Value: 0.8\n","State: (6, 6, False), Value: -0.3333333333333333\n","State: (11, 4, False), Value: 0.0\n","State: (16, 4, False), Value: -0.7142857142857143\n","State: (5, 10, False), Value: 0.0\n","State: (18, 2, True), Value: -1.0\n","State: (16, 9, False), Value: -0.6\n","State: (14, 2, True), Value: -1.0\n","State: (6, 3, False), Value: -1.0\n","State: (13, 6, False), Value: -1.0\n","State: (19, 3, True), Value: -1.0\n","State: (6, 9, False), Value: -1.0\n","State: (6, 5, False), Value: 0.0\n","State: (17, 6, True), Value: 0.5\n","State: (16, 6, True), Value: -0.3333333333333333\n","State: (15, 6, True), Value: -0.5\n","State: (15, 1, False), Value: -1.0\n","State: (20, 1, False), Value: -0.42857142857142855\n","State: (19, 5, False), Value: 0.0\n","State: (7, 4, False), Value: 0.3333333333333333\n","State: (12, 10, True), Value: -1.0\n","State: (18, 4, True), Value: -1.0\n","State: (11, 7, False), Value: -1.0\n","State: (19, 8, True), Value: 0.0\n","State: (16, 7, True), Value: -1.0\n","State: (13, 8, True), Value: 0.0\n","State: (18, 4, False), Value: 0.2\n","State: (16, 1, True), Value: -1.0\n","State: (6, 8, False), Value: -0.3333333333333333\n","State: (19, 6, True), Value: 0.0\n","State: (8, 1, False), Value: -1.0\n","State: (9, 4, False), Value: 0.0\n","State: (14, 1, True), Value: -1.0\n","State: (21, 5, False), Value: -0.5\n","State: (5, 8, False), Value: -1.0\n","State: (7, 10, False), Value: -1.0\n","State: (7, 9, False), Value: -1.0\n","State: (17, 4, True), Value: -1.0\n","State: (14, 5, True), Value: -1.0\n","State: (19, 1, True), Value: -1.0\n","State: (15, 1, True), Value: -1.0\n","State: (8, 9, False), Value: -1.0\n","State: (5, 7, False), Value: -1.0\n","State: (10, 6, False), Value: 0.0\n","State: (9, 8, False), Value: -1.0\n","State: (8, 3, False), Value: -1.0\n","State: (14, 4, True), Value: -1.0\n","State: (18, 5, True), Value: 1.0\n","State: (5, 4, False), Value: 0.0\n","State: (17, 10, True), Value: 0.2\n","State: (16, 2, True), Value: -1.0\n","State: (6, 4, False), Value: -1.0\n","State: (19, 5, True), Value: 0.5\n","State: (20, 9, True), Value: 1.0\n","State: (18, 3, True), Value: -1.0\n","State: (7, 3, False), Value: -1.0\n","State: (16, 4, True), Value: 1.0\n","State: (15, 9, True), Value: -1.0\n","State: (13, 1, True), Value: 1.0\n","State: (14, 7, True), Value: -1.0\n","State: (9, 9, False), Value: -1.0\n","State: (9, 6, False), Value: 1.0\n","State: (9, 7, False), Value: -0.5\n","State: (21, 3, True), Value: -1.0\n","State: (8, 8, False), Value: -1.0\n","State: (13, 3, True), Value: 1.0\n","State: (7, 2, False), Value: -1.0\n","State: (21, 5, True), Value: -0.5\n","State: (12, 5, True), Value: 0.0\n","State: (12, 6, True), Value: -1.0\n","State: (20, 5, True), Value: 0.0\n","State: (13, 4, True), Value: 0.0\n","State: (20, 8, True), Value: 1.0\n","State: (18, 9, True), Value: -1.0\n","State: (20, 3, True), Value: 1.0\n","State: (12, 3, True), Value: -1.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ar85VmS7e-En"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"78TVzhz4e-BJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MtaHVG-0e9-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JvGVX06me98D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AjqJv_xOe95h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"94-z9H6Be929"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rAt4XdtIe90a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c6-5r3T6e9x-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GgusPHYdVoa6","executionInfo":{"status":"ok","timestamp":1712089343587,"user_tz":-330,"elapsed":467,"user":{"displayName":"AMAN PARASHER (RA2111047010157)","userId":"12589665428718136377"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Monte Carlo prediction is a method used in reinforcement learning to estimate the value of being in a particular state under a given policy. In simple terms, it works by simulating many episodes of an agent interacting with the environment and averaging the observed returns (rewards) obtained from those episodes to estimate the value of each state.\n","\n","Here's a breakdown of how Monte Carlo prediction works:\n","\n","1. **Episode Simulation**: The agent interacts with the environment, starting from a given state, by taking actions according to a specified policy. This interaction continues until the episode terminates.\n","\n","2. **Return Calculation**: At the end of each episode, the total return (sum of rewards) obtained from that episode is calculated.\n","\n","3. **State Value Estimation**: For each state visited in the episode, the observed return is associated with that state. Over multiple episodes, these observed returns are averaged to estimate the value of each state.\n","\n","4. **Updating Estimates**: As more episodes are simulated, the estimates of state values are updated to become more accurate, reflecting the agent's learned understanding of the environment.\n","\n","Overall, Monte Carlo prediction provides a way to estimate the value of different states in an environment based on the rewards obtained by following a given policy over multiple episodes of interaction."],"metadata":{"id":"MQy1-luVe4Q7"}},{"cell_type":"code","source":[],"metadata":{"id":"D_L-Rhk8e4AR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code implements Monte Carlo prediction, a method for estimating the value function of a given policy in a reinforcement learning environment. Here's how it works:\n","\n","1. **Initialization**: It initializes the value function `V`, which maps states to their corresponding values. It also initializes dictionaries to keep track of the sum of returns and the count of visits to each state.\n","\n","2. **Episode Generation**: For each episode, it generates a complete episode by interacting with the environment. It starts by resetting the environment to its initial state and then iteratively takes actions according to a random policy until the episode terminates. During this process, it records the states visited and the rewards received at each time step.\n","\n","3. **Backward Update of Returns**: After completing an episode, it calculates the returns `G` for each state visited in reverse order. It does this by iteratively discounting future rewards and summing them up.\n","\n","4. **Update Value Function**: For each state visited in the episode, it updates the sum of returns and the count of visits. Then, it updates the value of each state by averaging the returns obtained.\n","\n","5. **Repeat**: Steps 2-4 are repeated for the specified number of episodes.\n","\n","6. **Return Value Function**: Finally, it returns the estimated value function `V`, which represents the expected cumulative reward that can be obtained starting from each state under the random policy.\n","\n","Overall, this algorithm iteratively learns the value of each state by simulating episodes, computing the returns obtained, and updating the value function accordingly. Through this process, it gradually improves its estimate of the state values, which can be useful for evaluating and improving policies in reinforcement learning tasks."],"metadata":{"id":"SxhyYS97eiyE"}},{"cell_type":"code","source":[],"metadata":{"id":"qq5ug0d8ej0q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the provided code snippet, 'Blackjack-v1' refers to the version 1 of the Blackjack environment in the OpenAI Gym toolkit.\n","\n","Blackjack is a popular card game where the player aims to have a hand value closer to 21 than the dealer's hand without exceeding it. The environment simulates this game, allowing agents to interact with it by choosing actions (e.g., hit or stand) based on the current state (e.g., the player's current hand and the dealer's visible card).\n","\n","In the Gym environment, 'Blackjack-v1' specifically refers to a variant of the Blackjack game where the player's hand is dealt two cards initially, and the dealer's first card is visible to the player. The goal of reinforcement learning algorithms applied to this environment might be to learn a policy that maximizes the expected return (e.g., winning as many hands as possible)."],"metadata":{"id":"iBz_Fansesfn"}},{"cell_type":"code","source":[],"metadata":{"id":"_FTKbY8CetHT"},"execution_count":null,"outputs":[]}]}