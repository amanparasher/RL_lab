{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPX8m4qlbZPsCdzO6lT1OUh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Implement the Monte Carlo Off-Policy Control with Importance Sampling.\n"],"metadata":{"id":"7lvICS12UMoP"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7BLkaagnUFBp","executionInfo":{"status":"ok","timestamp":1712087699278,"user_tz":-330,"elapsed":357,"user":{"displayName":"AMAN PARASHER (RA2111047010157)","userId":"12589665428718136377"}},"outputId":"b7c708e1-cc53-48e9-d29d-4bb55af316b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Q-values:\n","State: (21, 9, False)\n","Actions: [ 0. -1.]\n","State: (18, 10, False)\n","Actions: [-0.5  0. ]\n","State: (11, 10, False)\n","Actions: [0. 1.]\n","State: (13, 6, False)\n","Actions: [ 1. -1.]\n","State: (10, 10, False)\n","Actions: [-1.  1.]\n","State: (16, 8, False)\n","Actions: [-1. -1.]\n","State: (19, 7, False)\n","Actions: [1. 0.]\n","State: (18, 7, False)\n","Actions: [0.         0.33333333]\n","State: (19, 1, False)\n","Actions: [-0.33333333 -1.        ]\n","State: (16, 3, False)\n","Actions: [-1.  1.]\n","State: (17, 1, False)\n","Actions: [-1.  0.]\n","State: (12, 10, False)\n","Actions: [-0.33333333  1.        ]\n","State: (19, 1, True)\n","Actions: [ 0. -1.]\n","State: (17, 8, False)\n","Actions: [1. 0.]\n","State: (12, 6, False)\n","Actions: [0. 0.]\n","State: (20, 4, False)\n","Actions: [ 1. -1.]\n","State: (14, 6, False)\n","Actions: [-1. -1.]\n","State: (17, 6, False)\n","Actions: [0. 0.]\n","State: (10, 6, False)\n","Actions: [0. 0.]\n","State: (19, 10, False)\n","Actions: [ 0. -1.]\n","State: (13, 1, False)\n","Actions: [-1. -1.]\n","State: (20, 3, False)\n","Actions: [1. 0.]\n","State: (19, 5, True)\n","Actions: [-1.  0.]\n","State: (19, 5, False)\n","Actions: [-1.  0.]\n","State: (12, 5, False)\n","Actions: [ 0. -1.]\n","State: (12, 4, False)\n","Actions: [-1. -1.]\n","State: (19, 2, False)\n","Actions: [-1.  0.]\n","State: (12, 9, False)\n","Actions: [-1.  0.]\n","State: (20, 1, False)\n","Actions: [ 0.  -0.5]\n","State: (19, 9, False)\n","Actions: [ 1. -1.]\n","State: (12, 2, False)\n","Actions: [ 0. -1.]\n","State: (12, 7, False)\n","Actions: [1. 0.]\n","State: (8, 1, False)\n","Actions: [-1.  0.]\n","State: (10, 2, False)\n","Actions: [-1.  0.]\n","State: (20, 10, False)\n","Actions: [ 1. -1.]\n","State: (16, 1, False)\n","Actions: [ 0. -1.]\n","State: (21, 10, True)\n","Actions: [1. 0.]\n","State: (17, 9, False)\n","Actions: [-1.  0.]\n","State: (4, 8, False)\n","Actions: [-1.  0.]\n","State: (17, 10, False)\n","Actions: [ 0. -1.]\n","State: (11, 6, False)\n","Actions: [0. 0.]\n","State: (9, 6, False)\n","Actions: [ 0. -1.]\n","State: (15, 10, False)\n","Actions: [ 0.  -0.2]\n","State: (13, 9, False)\n","Actions: [ 0. -1.]\n","State: (13, 6, True)\n","Actions: [-1.  0.]\n","State: (16, 7, True)\n","Actions: [1. 0.]\n","State: (20, 6, False)\n","Actions: [1. 1.]\n","State: (15, 6, False)\n","Actions: [0. 1.]\n","State: (14, 3, False)\n","Actions: [1. 0.]\n","State: (14, 10, False)\n","Actions: [ 1. -1.]\n","State: (21, 1, False)\n","Actions: [0. 0.]\n","State: (12, 8, False)\n","Actions: [ 0. -1.]\n","State: (13, 7, False)\n","Actions: [-1.  0.]\n","State: (20, 8, False)\n","Actions: [1. 1.]\n","State: (14, 8, False)\n","Actions: [0.         0.33333333]\n","State: (21, 8, False)\n","Actions: [1. 0.]\n","State: (10, 8, False)\n","Actions: [-1.  0.]\n","State: (21, 4, True)\n","Actions: [1. 0.]\n","State: (16, 10, False)\n","Actions: [-1. -1.]\n","State: (10, 3, False)\n","Actions: [1. 0.]\n","State: (13, 5, False)\n","Actions: [ 0. -1.]\n","State: (18, 3, False)\n","Actions: [ 0. -1.]\n","State: (17, 2, False)\n","Actions: [ 0. -1.]\n","State: (15, 2, False)\n","Actions: [-1.  0.]\n","State: (12, 3, False)\n","Actions: [1. 0.]\n","State: (16, 9, False)\n","Actions: [0. 0.]\n","State: (21, 6, False)\n","Actions: [1. 0.]\n","State: (14, 1, False)\n","Actions: [ 0. -1.]\n","State: (13, 10, False)\n","Actions: [ 0. -1.]\n","State: (18, 9, False)\n","Actions: [ 0. -1.]\n","State: (20, 9, False)\n","Actions: [ 0. -1.]\n","State: (21, 5, False)\n","Actions: [1. 0.]\n","State: (16, 5, False)\n","Actions: [0. 1.]\n","State: (9, 10, False)\n","Actions: [-1.  0.]\n","\n","\n","Policy:\n","\n","State: (21, 9, False)\n","Policy: [1. 0.]\n","State: (18, 10, False)\n","Policy: [0. 1.]\n","State: (11, 10, False)\n","Policy: [0. 1.]\n","State: (13, 6, False)\n","Policy: [1. 0.]\n","State: (10, 10, False)\n","Policy: [0. 1.]\n","State: (16, 8, False)\n","Policy: [1. 0.]\n","State: (19, 7, False)\n","Policy: [1. 0.]\n","State: (18, 7, False)\n","Policy: [0. 1.]\n","State: (19, 1, False)\n","Policy: [1. 0.]\n","State: (16, 3, False)\n","Policy: [0. 1.]\n","State: (17, 1, False)\n","Policy: [0. 1.]\n","State: (12, 10, False)\n","Policy: [0. 1.]\n","State: (19, 1, True)\n","Policy: [1. 0.]\n","State: (17, 8, False)\n","Policy: [1. 0.]\n","State: (12, 6, False)\n","Policy: [1. 0.]\n","State: (20, 4, False)\n","Policy: [1. 0.]\n","State: (14, 6, False)\n","Policy: [1. 0.]\n","State: (17, 6, False)\n","Policy: [1. 0.]\n","State: (10, 6, False)\n","Policy: [1. 0.]\n","State: (19, 10, False)\n","Policy: [1. 0.]\n","State: (13, 1, False)\n","Policy: [1. 0.]\n","State: (20, 3, False)\n","Policy: [1. 0.]\n","State: (19, 5, True)\n","Policy: [0. 1.]\n","State: (19, 5, False)\n","Policy: [0. 1.]\n","State: (12, 5, False)\n","Policy: [1. 0.]\n","State: (12, 4, False)\n","Policy: [1. 0.]\n","State: (19, 2, False)\n","Policy: [0. 1.]\n","State: (12, 9, False)\n","Policy: [0. 1.]\n","State: (20, 1, False)\n","Policy: [1. 0.]\n","State: (19, 9, False)\n","Policy: [1. 0.]\n","State: (12, 2, False)\n","Policy: [1. 0.]\n","State: (12, 7, False)\n","Policy: [1. 0.]\n","State: (8, 1, False)\n","Policy: [0. 1.]\n","State: (10, 2, False)\n","Policy: [0. 1.]\n","State: (20, 10, False)\n","Policy: [1. 0.]\n","State: (16, 1, False)\n","Policy: [1. 0.]\n","State: (21, 10, True)\n","Policy: [1. 0.]\n","State: (17, 9, False)\n","Policy: [0. 1.]\n","State: (4, 8, False)\n","Policy: [0. 1.]\n","State: (17, 10, False)\n","Policy: [1. 0.]\n","State: (11, 6, False)\n","Policy: [1. 0.]\n","State: (9, 6, False)\n","Policy: [1. 0.]\n","State: (15, 10, False)\n","Policy: [1. 0.]\n","State: (13, 9, False)\n","Policy: [1. 0.]\n","State: (13, 6, True)\n","Policy: [0. 1.]\n","State: (16, 7, True)\n","Policy: [1. 0.]\n","State: (20, 6, False)\n","Policy: [1. 0.]\n","State: (15, 6, False)\n","Policy: [0. 1.]\n","State: (14, 3, False)\n","Policy: [1. 0.]\n","State: (14, 10, False)\n","Policy: [1. 0.]\n","State: (21, 1, False)\n","Policy: [1. 0.]\n","State: (12, 8, False)\n","Policy: [1. 0.]\n","State: (13, 7, False)\n","Policy: [0. 1.]\n","State: (20, 8, False)\n","Policy: [1. 0.]\n","State: (14, 8, False)\n","Policy: [0. 1.]\n","State: (21, 8, False)\n","Policy: [1. 0.]\n","State: (10, 8, False)\n","Policy: [0. 1.]\n","State: (21, 4, True)\n","Policy: [1. 0.]\n","State: (16, 10, False)\n","Policy: [1. 0.]\n","State: (10, 3, False)\n","Policy: [1. 0.]\n","State: (13, 5, False)\n","Policy: [1. 0.]\n","State: (18, 3, False)\n","Policy: [1. 0.]\n","State: (17, 2, False)\n","Policy: [1. 0.]\n","State: (15, 2, False)\n","Policy: [0. 1.]\n","State: (12, 3, False)\n","Policy: [1. 0.]\n","State: (16, 9, False)\n","Policy: [1. 0.]\n","State: (21, 6, False)\n","Policy: [1. 0.]\n","State: (14, 1, False)\n","Policy: [1. 0.]\n","State: (13, 10, False)\n","Policy: [1. 0.]\n","State: (18, 9, False)\n","Policy: [1. 0.]\n","State: (20, 9, False)\n","Policy: [1. 0.]\n","State: (21, 5, False)\n","Policy: [1. 0.]\n","State: (16, 5, False)\n","Policy: [0. 1.]\n","State: (9, 10, False)\n","Policy: [0. 1.]\n"]}],"source":["import numpy as np\n","import gym\n","from collections import defaultdict\n","\n","def generate_behavior_policy(env):\n","    num_actions = env.action_space.n\n","    if isinstance(env.observation_space, gym.spaces.Tuple):\n","        num_states = [space.n for space in env.observation_space.spaces]\n","        return defaultdict(lambda: np.ones(num_actions) / num_actions)\n","    else:\n","        return defaultdict(lambda: np.ones((num_actions)) / num_actions)\n","\n","def off_policy_mc_control(env, num_episodes, discount_factor=1.0):\n","    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n","    C = defaultdict(lambda: np.zeros(env.action_space.n))\n","    target_policy = defaultdict(lambda: np.zeros(env.action_space.n))\n","    behavior_policy = generate_behavior_policy(env)\n","\n","    for episode in range(num_episodes):\n","        episode_states = []\n","        episode_actions = []\n","        episode_rewards = []\n","\n","        state = env.reset()\n","        done = False\n","        while not done:\n","            action_probs = behavior_policy[state]\n","            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n","            next_state, reward, done, _ = env.step(action)\n","            episode_states.append(state)\n","            episode_actions.append(action)\n","            episode_rewards.append(reward)\n","            state = next_state\n","\n","        G = 0\n","        W = 1\n","        for t in range(len(episode_states) - 1, -1, -1):\n","            state = episode_states[t]\n","            action = episode_actions[t]\n","            reward = episode_rewards[t]\n","            G = discount_factor * G + reward\n","            C[state][action] += W\n","            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n","\n","            if action != np.argmax(target_policy[state]):\n","                break\n","            W /= behavior_policy[state][action]\n","\n","            if W == 0:\n","                break\n","\n","    for state in Q:\n","        target_policy[state] = np.zeros(env.action_space.n)\n","        target_policy[state][np.argmax(Q[state])] = 1.0\n","\n","    return Q, target_policy\n","\n","# Example usage\n","env = gym.make('Blackjack-v1')\n","num_episodes = 100\n","Q, policy = off_policy_mc_control(env, num_episodes)\n","# Print Q-values in a structured format\n","print(\"Q-values:\")\n","for state, values in Q.items():\n","    print(\"State:\", state)\n","    print(\"Actions:\", values)\n","\n","# Print policy in a structured format\n","print(\"\\n\\nPolicy:\\n\")\n","for state, actions in policy.items():\n","    print(\"State:\", state)\n","    print(\"Policy:\", actions)"]},{"cell_type":"markdown","source":["https://chat.openai.com/share/a22504cf-f8a2-4c64-874e-56d3a8ea7b35"],"metadata":{"id":"szUPsaKggndc"}},{"cell_type":"markdown","source":["1. **Initialize**: Set up Q-table, C-table, and target policy dictionaries.\n","\n","2. **Generate Policy**: Create a behavior policy for exploration.\n","\n","3. **Loop Episodes**:\n","   - Generate episodes using the behavior policy.\n","   - Calculate returns (G) for each state-action pair.\n","   - Update Q-table and C-table with returns using importance sampling.\n","\n","4. **Update Policy**: Adjust the target policy based on the updated Q-values.\n","\n","5. **Return**: Provide the learned Q-values and target policy."],"metadata":{"id":"VZpnIda-fw0J"}},{"cell_type":"markdown","source":["This code implements Monte Carlo Off-Policy Control with Importance Sampling for solving reinforcement learning problems. Here's a breakdown:\n","\n","1. **Environment Setup**: It imports necessary libraries and initializes the Blackjack environment from OpenAI Gym.\n","\n","2. **Policy Initialization**: The `generate_behavior_policy` function initializes a behavior policy, which dictates how the agent selects actions during exploration. This policy is initialized uniformly at the beginning.\n","\n","3. **Off-Policy Monte Carlo Control**: The `off_policy_mc_control` function performs off-policy Monte Carlo control. It iterates through a fixed number of episodes and collects experiences while following the behavior policy.\n","\n","4. **Episode Generation and Importance Sampling**: During each episode, the agent selects actions based on the behavior policy and updates its estimates of state-action values (Q-table) using importance sampling. This allows the agent to learn from experiences even when following a different policy than the target policy.\n","\n","5. **Policy Update**: After each episode, the target policy is updated based on the learned Q-values. The target policy tends to favor actions with higher estimated returns.\n","\n","6. **Output**: Finally, the learned Q-values and target policy are returned for evaluation or further use.\n","\n","This algorithm enables an agent to learn an optimal policy for the given environment, even when the exploration policy (behavior policy) is different from the desired target policy."],"metadata":{"id":"-D65lG8ogSec"}},{"cell_type":"code","source":[],"metadata":{"id":"pcbm2tLcgSMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0Qfa1D3IYVa-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Implement SARSA (On Policy TD Learning)"],"metadata":{"id":"vLR7poz1YU-H"}},{"cell_type":"markdown","source":["1. **Initialize Q-Table**: Start with a Q-table of zeros for all state-action pairs.\n","\n","2. **For Each Episode**:\n","   - Reset the environment.\n","   - Choose an action using epsilon-greedy policy based on Q-values.\n","   - Take the action, observe the next state and reward.\n","   - Choose the next action using epsilon-greedy policy.\n","   - Update Q-values using SARSA update rule.\n","   - Repeat until episode ends.\n","\n","3. **Policy Evaluation**: After episodes, Q-table approximates optimal state-action values.\n","\n","4. **Return**: Learned Q-values, representing expected returns for actions in each state."],"metadata":{"id":"Xrjys7B-gKrQ"}},{"cell_type":"code","source":["import numpy as np\n","import gym\n","\n","def epsilon_greedy_policy(Q, state, epsilon):\n","    num_actions = Q.shape[1]\n","    if np.random.rand() < epsilon:\n","        return np.random.choice(num_actions)\n","    else:\n","        return np.argmax(Q[state])\n","\n","def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n","    Q = np.zeros((env.observation_space.n, env.action_space.n))\n","\n","    for episode in range(num_episodes):\n","        state = env.reset()\n","        action = epsilon_greedy_policy(Q, state, epsilon)\n","        done = False\n","\n","        while not done:\n","            next_state, reward, done, _ = env.step(action)\n","            next_action = epsilon_greedy_policy(Q, next_state, epsilon)\n","            td_target = reward + discount_factor * Q[next_state][next_action]\n","            td_error = td_target - Q[state][action]\n","            Q[state][action] += alpha * td_error\n","\n","            state = next_state\n","            action = next_action\n","\n","    return Q\n","\n","# Example usage\n","env = gym.make('FrozenLake-v1')\n","num_episodes = 10000\n","Q = sarsa(env, num_episodes)\n","print(\"Q-values:\", Q)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbOH6HQNXFC3","executionInfo":{"status":"ok","timestamp":1712087403156,"user_tz":-330,"elapsed":5241,"user":{"displayName":"AMAN PARASHER (RA2111047010157)","userId":"12589665428718136377"}},"outputId":"0355fa08-9743-4230-b3c4-3f0b22a1e364"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]},{"output_type":"stream","name":"stdout","text":["Q-values: [[0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.03125 0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.03125 0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.1875  0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.      0.     ]\n"," [0.      0.      0.75    0.     ]\n"," [0.      0.      0.      0.     ]]\n"]}]},{"cell_type":"markdown","source":["This code implements the SARSA (State-Action-Reward-State-Action) algorithm, a form of on-policy Temporal Difference (TD) learning. Here's an explanation of the key components:\n","\n","1. **epsilon_greedy_policy**: This function defines an epsilon-greedy policy, which selects a random action with probability epsilon, and the action with the highest Q-value with probability (1 - epsilon).\n","\n","2. **sarsa**: This function initializes a Q-table with zeros and iterates through a specified number of episodes. Within each episode:\n","   - It resets the environment and selects the initial action based on the epsilon-greedy policy.\n","   - It iterates through steps within the episode, updating the Q-values using the SARSA update rule: \\( Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot (r + \\gamma \\cdot Q(s', a') - Q(s, a)) \\), where \\( \\alpha \\) is the learning rate, \\( r \\) is the reward, \\( \\gamma \\) is the discount factor, \\( s \\) is the current state, \\( a \\) is the current action, \\( s' \\) is the next state, and \\( a' \\) is the next action.\n","   - The current state and action are updated for the next iteration based on the environment's response.\n","\n","3. **Example usage**: It creates an environment (in this case, FrozenLake), runs SARSA for a specified number of episodes, and prints the learned Q-values.\n","\n","Overall, SARSA learns an optimal policy by iteratively updating Q-values based on observed transitions and rewards, aiming to maximize expected cumulative rewards while following a given policy."],"metadata":{"id":"uaQZOOBPga2u"}},{"cell_type":"code","source":[],"metadata":{"id":"8hsiai4xXFvE"},"execution_count":null,"outputs":[]}]}