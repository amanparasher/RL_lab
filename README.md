# Reinforcement Learning Experiments

This repository contains implementations of various reinforcement learning algorithms. The experiments conducted include:

## Q-Learning (Off Policy TD Learning)
- Q-Learning is an off-policy Temporal Difference (TD) learning algorithm used to learn the optimal action-value function.

## Monte Carlo Off-Policy Control with Importance Sampling
- This experiment implements Monte Carlo Off-Policy Control, which is a method to learn the optimal policy using Monte Carlo sampling with importance sampling.

## SARSA (On Policy TD Learning)
- SARSA (State-Action-Reward-State-Action) is an on-policy Temporal Difference (TD) learning algorithm used to learn the optimal policy.

## Monte Carlo Prediction
- Monte Carlo Prediction is a method to estimate the value function of a policy by averaging sample returns.

## Policy Improvement and Value Iteration
- This experiment involves dynamic programming algorithms such as Policy Improvement and Value Iteration used for solving Markov Decision Processes (MDPs).

## Policy Evaluation and Policy Iteration
- Policy Evaluation and Policy Iteration are dynamic programming methods used to compute the value function and improve the policy iteratively.

## Dynamic Programming Algorithms for Solving MDPs
- The repository includes implementations of various dynamic programming algorithms for solving MDPs, including Policy Improvement, Value Iteration, Policy Evaluation, and Policy Iteration.

## Multi-Armed Bandit (MAB)
- The Multi-Armed Bandit (MAB) problem is addressed, which involves finding the optimal strategy for allocating resources among multiple actions with uncertain rewards.

## Armed Bandit Problem
- The Armed Bandit Problem, a simpler version of the MAB problem with a single state, is addressed using various algorithms.

Each experiment is accompanied by Python code implementing the respective algorithm. These implementations serve as educational resources and can be used as building blocks for more complex reinforcement learning projects.

Please refer to individual experiment directories for detailed explanations and usage instructions.

**Note:** These implementations are for educational purposes and may not be optimized for performance or production use.
