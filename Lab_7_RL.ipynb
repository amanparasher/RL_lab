{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTaUgPSrhRXwSIhkAJEyqH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZSHt8iuVJGs","executionInfo":{"status":"ok","timestamp":1709872392167,"user_tz":-330,"elapsed":358,"user":{"displayName":"AMAN PARASHER (RA2111047010157)","userId":"12589665428718136377"}},"outputId":"b7233c4c-ecac-4bfe-8b6f-4922775b21b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Value Function:\n","[18.10809938 19.99999128 18.91891107]\n","\n","Optimal Policy:\n","[0 0 0]\n","\n","Policy is stable.\n"]}],"source":["import numpy as np\n","\n","# Define the MDP parameters\n","num_states = 3\n","num_actions = 2\n","gamma = 0.9  # discount factor\n","\n","# Define the transition probabilities and rewards\n","# Transitions are represented as a 3D array: P[state, action, next_state]\n","P = np.array([[[0.7, 0.3, 0.0], [0.0, 0.8, 0.2]],  # state 0\n","              [[0.0, 1.0, 0.0], [0.1, 0.0, 0.9]],  # state 1\n","              [[0.4, 0.6, 0.0], [0.0, 0.0, 1.0]]])  # state 2\n","\n","# Rewards are represented as a 2D array: R[state, action]\n","R = np.array([[1.0, -1.0],  # state 0\n","              [2.0, 0.0],    # state 1\n","              [0.0, 1.0]])   # state 2\n","\n","# Initialize the value function and policy\n","V = np.zeros(num_states)\n","policy = np.zeros(num_states, dtype=int)\n","\n","# Policy Improvement\n","def policy_improvement(V, policy):\n","    policy_stable = True\n","    for s in range(num_states):\n","        old_action = policy[s]\n","        action_values = np.zeros(num_actions)\n","        for a in range(num_actions):\n","            action_values[a] = np.sum(P[s, a, :] * (R[:, a] + gamma * V[:]))\n","        policy[s] = np.argmax(action_values)\n","        if old_action != policy[s]:\n","            policy_stable = False\n","    return policy_stable\n","\n","# Value Iteration\n","def value_iteration():\n","    epsilon = 1e-6\n","    while True:\n","        delta = 0\n","        for s in range(num_states):\n","            v = V[s]\n","            action_values = np.zeros(num_actions)\n","            for a in range(num_actions):\n","                action_values[a] = np.sum(P[s, a, :] * (R[:, a] + gamma * V[:]))\n","            V[s] = np.max(action_values)\n","            delta = max(delta, np.abs(v - V[s]))\n","        if delta < epsilon:\n","            break\n","\n","# Run Value Iteration\n","value_iteration()\n","\n","# Run Policy Improvement\n","policy_stable = policy_improvement(V, policy)\n","\n","# Display the results\n","print(\"Optimal Value Function:\")\n","print(V)\n","print(\"\\nOptimal Policy:\")\n","print(policy)\n","\n","if policy_stable:\n","    print(\"\\nPolicy is stable.\")\n","else:\n","    print(\"\\nPolicy is not stable.\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"y0lP6i2oViLG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code implements the Value Iteration algorithm for solving Markov Decision Processes (MDPs) and performs policy improvement to find the optimal policy. Let's break it down:\n","\n","1. **MDP Parameters**:\n","   - The code defines the parameters of the MDP: the number of states (\\( S \\)), the number of actions (\\( A \\)), and the discount factor (\\( \\gamma \\)).\n","\n","2. **Transition Probabilities and Rewards**:\n","   - The transition probabilities (\\( P \\)) and rewards (\\( R \\)) matrices are defined.\n","   - Transitions are represented as a 3D array where \\( P[state, action, next\\_state] \\) gives the probability of transitioning from a state to the next state given an action.\n","   - Rewards are represented as a 2D array where \\( R[state, action] \\) gives the immediate reward for taking an action in a particular state.\n","\n","3. **Policy Initialization**:\n","   - The value function (\\( V \\)) and policy (\\( \\pi \\)) are initialized with zeros.\n","\n","4. **Policy Improvement Function**:\n","   - The policy_improvement() function updates the policy based on the current value function.\n","   - It iterates through each state and selects the action that maximizes the expected cumulative reward.\n","   - If the policy remains unchanged after an iteration, it indicates policy stability.\n","\n","5. **Value Iteration Function**:\n","   - The value_iteration() function performs the value iteration algorithm to compute the optimal value function.\n","   - It iteratively updates the value function until convergence by applying the Bellman optimality equation.\n","\n","6. **Run Value Iteration**:\n","   - The value_iteration() function is called to compute the optimal value function.\n","\n","7. **Run Policy Improvement**:\n","   - The policy_improvement() function is called to update the policy based on the computed value function.\n","\n","8. **Display Results**:\n","   - The optimal value function (\\( V \\)) and policy (\\( \\pi \\)) are printed.\n","   - It also checks if the policy is stable or not.\n","\n","This code provides a complete implementation of the Value Iteration algorithm for finding the optimal policy in an MDP and demonstrates its usage with a simple example."],"metadata":{"id":"a2e3sb8yf7YB"}},{"cell_type":"code","source":[],"metadata":{"id":"0LtR44nof7_Y"},"execution_count":null,"outputs":[]}]}