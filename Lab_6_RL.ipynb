{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOzbGKMI/5zCWGnEIePOYVx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","\n","def policy_evaluation(policy, P, R, gamma, theta=1e-6):\n","    S, A = policy.shape\n","    V = np.zeros(S)\n","\n","    while True:\n","        delta = 0\n","        for s in range(S):\n","            v_old = V[s]\n","            a = np.argmax(policy[s])  # Deterministic policy\n","            V[s] = R[s, a] + gamma * np.sum(P[s, a, :] * V)\n","            delta = max(delta, np.abs(v_old - V[s]))\n","\n","        if delta < theta:\n","            break\n","\n","    return V\n","\n","def policy_iteration(P, R, gamma=0.9, max_iter=100, tol=1e-6):\n","    S, A, _ = P.shape\n","\n","    # Initialize policy\n","    policy = np.ones((S, A)) / A\n","\n","    for _ in range(max_iter):\n","        # Policy Evaluation\n","        V = policy_evaluation(policy, P, R, gamma)\n","\n","        # Policy Improvement\n","        policy_stable = True\n","        for s in range(S):\n","            old_action = np.argmax(policy[s])\n","            action_values = np.zeros(A)\n","            for a in range(A):\n","                action_values[a] = R[s, a] + gamma * np.sum(P[s, a, :] * V)\n","            best_action = np.argmax(action_values)\n","            policy[s] = np.eye(A)[best_action]\n","\n","            if old_action != best_action:\n","                policy_stable = False\n","\n","        if policy_stable:\n","            break\n","\n","    return policy\n","\n","# Example Usage:\n","# Define the MDP parameters\n","num_states = 3\n","num_actions = 2\n","gamma = 0.9\n","\n","# Define the transition probabilities and rewards\n","P = np.array([[[0.7, 0.3, 0.0], [0.0, 0.8, 0.2]],  # state 0\n","              [[0.0, 1.0, 0.0], [0.1, 0.0, 0.9]],  # state 1\n","              [[0.4, 0.6, 0.0], [0.0, 0.0, 1.0]]])  # state 2\n","\n","R = np.array([[1.0, -1.0],  # state 0\n","              [2.0, 0.0],    # state 1\n","              [0.0, 1.0]])   # state 2\n","\n","# Run Policy Iteration\n","optimal_policy = policy_iteration(P, R, gamma)\n","\n","# Display the results\n","print(\"Optimal Policy:\")\n","print(np.argmax(optimal_policy, axis=1))\n"],"metadata":{"id":"MSwaBEjRc3DH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709875339590,"user_tz":-330,"elapsed":378,"user":{"displayName":"AMAN PARASHER (RA2111047010157)","userId":"12589665428718136377"}},"outputId":"6ae5a442-fb8a-4de3-ca93-3954231bbd66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Policy:\n","[0 0 0]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"v5fWYZnLfumv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code implements the Policy Iteration algorithm for solving Markov Decision Processes (MDPs). Let's break it down:\n","\n","1. **policy_evaluation() Function**:\n","   - This function evaluates the value function \\( V \\) for a given policy.\n","   - It takes the policy, transition probabilities \\( P \\), rewards \\( R \\), discount factor \\( \\gamma \\), and a convergence threshold \\( \\theta \\) as inputs.\n","   - It iteratively updates the value function until convergence using the Bellman equation for the given policy.\n","   - The value function \\( V \\) is updated for each state \\( s \\) by considering the action chosen by the policy, the immediate reward, and the expected future rewards.\n","   - It returns the value function \\( V \\) for the given policy.\n","\n","2. **policy_iteration() Function**:\n","   - This function implements the Policy Iteration algorithm.\n","   - It takes transition probabilities \\( P \\), rewards \\( R \\), discount factor \\( \\gamma \\), maximum number of iterations \\( max\\_iter \\), and a tolerance \\( tol \\) as inputs.\n","   - It initializes a random policy and iteratively performs policy evaluation and improvement until convergence.\n","   - In policy evaluation, it computes the value function for the current policy using the policy_evaluation() function.\n","   - In policy improvement, it updates the policy by selecting the action that maximizes the expected cumulative reward for each state.\n","   - It terminates when the policy becomes stable, i.e., no changes occur in the policy during policy improvement.\n","   - It returns the optimal policy.\n","\n","3. **Example Usage**:\n","   - It defines the parameters for a simple MDP: number of states (\\( S \\)), number of actions (\\( A \\)), and discount factor (\\( \\gamma \\)).\n","   - It defines the transition probabilities (\\( P \\)) and rewards (\\( R \\)) matrices for the MDP.\n","   - It runs the Policy Iteration algorithm using the defined MDP parameters to find the optimal policy.\n","   - It prints the optimal policy, showing the action to be taken in each state.\n","\n","This code provides a framework for solving MDPs using the Policy Iteration algorithm and demonstrates its usage with a simple example."],"metadata":{"id":"akhYoCLTfvdb"}},{"cell_type":"code","source":[],"metadata":{"id":"E7JzQeK6glrf"},"execution_count":null,"outputs":[]}]}