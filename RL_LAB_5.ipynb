{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhg2GOQULNPPXIUYzq5MF6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zws4bmt-bTpG","executionInfo":{"status":"ok","timestamp":1708666240291,"user_tz":-330,"elapsed":4,"user":{"displayName":"AMAN PARASHER (RA2111047010157)","userId":"12589665428718136377"}},"outputId":"db4d9c41-d41c-4824-ea4d-2ca22bbba657"},"outputs":[{"output_type":"stream","name":"stdout","text":["Value function: [4179380.69718659 3962077.1204357 ]\n","Optimal policy: [[1 1]\n"," [0 0]]\n"]}],"source":["import numpy as np\n","\n","def policy_iteration(P, R, gamma=0.9, max_iter=100, tol=1e-6):\n","\n","    S, A = R.shape  # Number of states and actions\n","\n","    # Initialize value function and policy\n","    V = np.zeros(S)\n","    pi = np.zeros((S, A), dtype=int)\n","\n","    for _ in range(max_iter):\n","        # Policy evaluation\n","        delta = 0\n","        for i in range(S):\n","            v_old = V[i]\n","            V[i] = max(R[i, a] + gamma * np.dot(P[i, :, a], V) for a in range(A))\n","            delta = max(delta, abs(v_old - V[i]))\n","\n","        if delta < tol:\n","            break  # Converged\n","\n","        # Policy improvement\n","        for i in range(S):\n","            pi[i] = np.argmax(R[i, :] + gamma * np.dot(P[i, :, :], V))\n","\n","    return V, pi\n","\n","# Example usage with a simple MDP (adapt matrices as needed)\n","P = np.array([[[0.5, 0.5], [0.8, 0.2]], [[0.7, 0.3], [0.1, 0.9]]])\n","R = np.array([[1, 0], [0, 2]])\n","gamma = 0.9\n","\n","V, pi = policy_iteration(P, R, gamma)\n","\n","print(\"Value function:\", V)\n","print(\"Optimal policy:\", pi)\n","\n"]},{"cell_type":"markdown","source":["\"\"\"\n","    Policy iteration algorithm for solving an MDP.\n","\n","    Args:\n","        P: Transition probability matrix (S x S x A).\n","        R: Reward matrix (S x A).\n","        gamma: Discount factor (0 < gamma < 1).\n","        max_iter: Maximum number of iterations.\n","        tol: Tolerance for convergence.\n","\n","    Returns:\n","        V: Value function (S x 1).\n","        pi: Optimal policy (S x A).\n","    \"\"\""],"metadata":{"id":"4jACYnZlcjCl"}},{"cell_type":"markdown","source":["This Python code implements the Policy Iteration algorithm for solving Markov Decision Processes (MDPs). Here's a breakdown of the code:\n","\n","1. **Importing Libraries**: The code imports NumPy, a library for numerical computing in Python.\n","\n","2. **policy_iteration() Function**: This function performs policy iteration to find the optimal policy for the given MDP. It takes transition probabilities (P), rewards (R), discount factor (gamma), maximum number of iterations (max_iter), and tolerance for convergence (tol) as inputs.\n","\n","3. **Initialization**: It initializes the value function (V) and policy (pi) arrays with zeros.\n","\n","4. **Policy Evaluation**: It iteratively evaluates the value function using the Bellman equation until convergence. The value function is updated for each state (i) by considering all possible actions (a) and calculating the maximum expected future reward.\n","\n","5. **Policy Improvement**: It updates the policy based on the updated value function. For each state (i), it selects the action that maximizes the expected future reward.\n","\n","6. **Convergence Check**: It checks for convergence by comparing the change in value function (delta) with the specified tolerance (tol). If the change is below the tolerance, the algorithm breaks, indicating convergence.\n","\n","7. **Return**: The function returns the optimal value function (V) and policy (pi).\n","\n","8. **Example Usage**: An example usage of the policy_iteration() function is provided with a simple MDP defined by transition probabilities (P) and rewards (R).\n","\n","9. **Print Results**: The optimal value function (V) and policy (pi) are printed.\n","\n","Overall, this code demonstrates how to apply the Policy Iteration algorithm to find the optimal policy for a given Markov Decision Process.\n","\n","\n","In the provided code, MDP stands for Markov Decision Process.\n","\n","A Markov Decision Process (MDP) is a mathematical framework used to model decision-making problems in which an agent interacts with an environment over a series of discrete time steps.\n","\n","In an MDP:\n","- The agent exists in a set of states \\( S \\), and at each time step, it chooses an action \\( A \\) from a set of possible actions available in that state.\n","- The environment transitions the agent from its current state to a new state based on the chosen action. These transitions are probabilistic and governed by transition probabilities \\( P(s'|s, a) \\), which represent the probability of transitioning to state \\( s' \\) from state \\( s \\) when action \\( a \\) is taken.\n","- Upon transitioning to a new state, the agent receives a reward \\( R(s, a, s') \\), which represents the immediate benefit or cost associated with the transition.\n","\n","The objective in an MDP is to find a policy \\( \\pi \\), which is a mapping from states to actions, that maximizes the expected cumulative reward over time.\n","\n","In the code:\n","- The transition probabilities \\( P \\) and rewards \\( R \\) are provided as input to the policy_iteration() function, representing the dynamics of the MDP.\n","- The policy_iteration() function computes the optimal policy \\( \\pi \\) and value function \\( V \\) for the given MDP, which allows the agent to make decisions that maximize its long-term rewards."],"metadata":{"id":"I57VlLeffdNb"}},{"cell_type":"code","source":[],"metadata":{"id":"KuemM1g2hVXR"},"execution_count":null,"outputs":[]}]}